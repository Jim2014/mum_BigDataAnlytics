cText Mining
Dr. Anil Maheshwari
Learning Objectives
Differentiate between text mining and data mining
Understand text mining objectives and  application areas
Understand the text mining process
Dimensions of Data/Text/Web Mining
DTW Mining Inputs
Data Domains (industry, function, etc)
Types of Data field (categorical, numerical, text, blobs)
Data sources (operations, web, office communictions)
Data quality (missing values, outliers)
DTW Mining Outputs/Goals
Objective functions (prediction, cluster, sentiment, etc)
Output description types (trees, rules, prediction models)
Data representation types
DTW Mining Processes
Methods (Classification, Clustering, Associations, Sequences)
Statistical vs AI machine learning
Algorithm types (decision, trees, rules, neural net, etc)
Reliability/Accuracy of results (ROC, Confusion matrix, etc) 

Data Mining versus Text Mining
Both seek for novel and useful patterns
Both are semi-automated processes
Difference is the nature of the data: 
Data Mining works on structured data stored in databases
Text Mining works on unstructured data in Word documents, PDF files, XML files, etc
Text mining – first, impose structure to the data, then mine the structured data 
Text Mining Concepts
Text mining Objective
A semi-automated process of extracting knowledge from unstructured data sources
i.e. knowledge discovery in textual databases
Structuring a collection of text
Traditional approach: bag-of-words
New approach: natural language processing for understanding nuances of spoken words
Sentiment Analysis
A technique used to detect favorable and unfavorable opinions toward specific products and services 

Text Mining Concepts
Numerous rich sources of text for mining
e.g., law (court orders), academic research (research articles), finance (quarterly reports), medicine (discharge summaries), biology (molecular interactions), technology (patent files), marketing (customer comments), etc.
Text Mining Applications
Marketing applications, e.g. Enables better CRM
Security applications, e.g. Deception detection
Medicine and biology, e.g. Literature-based gene identification
Academic applications, e.g. Research stream analysis


Context for Text Mining Process
Text Mining Process – three steps
Text Mining Process
Step 1: Establish the corpus
Collect all relevant unstructured data          (e.g., textual documents, XML files, emails, Web pages, short notes, voice recordings…)
Digitize, standardize the collection              (e.g., all in ASCII text files)
Place the collection in a common place        (e.g., in a flat file, or in a directory as separate files) 

Text Mining Process
Step 2: Create the Term–by–Document Matrix
Text Mining Process
Step 2: Create the Term–by–Document Matrix (TDM), cont.
Should all terms be included?
Stop words, include words
Synonyms, homonyms
Stemming
What is the best representation of the indices (values in cells)? 
Row counts; binary frequencies; log frequencies;
Inverse document frequency
Text Mining Process
Step 2: Create the Term–by–Document Matrix (TDM), cont.
TDM is a sparse matrix. How can we reduce the dimensionality of the TDM?
Manual - a domain expert goes through it
Eliminate terms with very few occurrences in very few documents 
Transform the matrix using singular value decomposition (SVD) 
SVD is similar to principle component analysis 
Phrase-Mining and Term-Mining
Text Mining Process
Step 3: Extract patterns/knowledge
Classification (text categorization)
Clustering (natural groupings of text)
Improve search recall & precision
Scatter/gather
Query-specific clustering
Association rules among the documents
Trend Analysis 



Web Mining
Dr. Anil Maheshwari
Learning Objectives
Differentiate between Web mining and data mining
Understand Web mining, its objectives, and its three branches
Web content mining
Web structure mining
Web usage mining

Web Mining
Web mining (or Web data mining) is the process of discovering intrinsic relationships from Web data (textual, linkage, or usage)
Web Content Mining
Mining of the textual content on the Web
Data is retrieved through web crawlers
Crawlers are primed to avoid traps
Using Hadoop to store data
Organizing webpages by search terms
Reverse Index Retrieval process for searching pages
Using MapReduce to quickly search and display pages
Web Structure Mining
Web pages include hyperlinks
Data collection is done using Web crawlers
Computing website importance
hyperlink-induced topic search (HITS) algorithm computes links into and out of a website
Web page types
Authoritative pages 
Many other pages link to it
Hub pages
Links to many other pages
PageRank 
What is PageRank
Relative importance of a page
Used in prioritizing pages for display/relevance
Calculating PageRank
Flow of Influence from pages pointing to it
Adjusted regularly to avoid gaming
Web Usage Mining
Extraction of information from data generated through Web page visits and transactions…
data stored in server access logs, referrer logs, agent logs, and client-side cookies
user characteristics and usage profiles
metadata, such as page attributes, content attributes, and usage data
Analysis of Clickstream data can reveal interesting patterns
Web Usage Mining
Web usage mining applications
Determine the lifetime value of clients
Design cross-marketing strategies across products.
Evaluate promotional campaigns
Target electronic ads and coupons at user groups based on user access patterns
Predict user behavior based on previously learned rules and users' profiles
Present dynamic information to users based on their interests and profiles…
Web Usage Mining (clickstream analysis)
Web Mining Success Stories
Amazon.com, Ask.com, Scholastic.com, …
Website Optimization Ecosystem

Thank you.
BACKUP
Link Analysis 
Introduction
Social network analysis
Co-citation and bibliographic coupling 
PageRank
HITS
Summary
Introduction
Early search engines mainly compare content similarity of the query and the indexed pages. I.e., 
They use information retrieval methods, cosine, TF-IDF, ... 
From 1996, it became clear that content similarity alone was no longer sufficient. 
The number of pages grew rapidly in the mid-late 1990’s. 
Try “classification technique”, Google estimates: 10 million relevant pages. 
How to choose only 30-40 pages and rank them suitably to present to the user?
Content similarity is easily spammed. 
A page owner can repeat some words and add many related words to boost the rankings of his pages and/or to make the pages relevant to a large number of queries. 
Introduction (cont …)
Starting around 1996, researchers began to work on the problem. They resort to hyperlinks. 
In Feb, 1997, Yanhong Li (Robin Li), Scotch Plains, NJ, filed a hyperlink based search patent. The method uses words in anchor text of hyperlinks.
Web pages on the other hand are connected through hyperlinks, which carry important information. 
Some hyperlinks: organize information at the same site. 
Other hyperlinks: point to pages from other Web sites. Such out-going hyperlinks often indicate an implicit conveyance of authority to the pages being pointed to. 
Those pages that are pointed to by many other pages are likely to contain authoritative information. 
Introduction (cont …)
During 1997-1998, two most influential hyperlink based search algorithms PageRank and HITS were reported. 
Both algorithms are related to social networks. They exploit the hyperlinks of the Web to rank pages according to their levels of “prestige” or “authority”. 
HITS: Jon Kleinberg (Cornel University), at Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, January 1998
PageRank: Sergey Brin and Larry Page, PhD students from Stanford University, at Seventh International World Wide Web Conference (WWW7) in April, 1998. 
PageRank powers the Google search engine. 
Introduction (cont …)
Apart from search ranking, hyperlinks are also useful for finding Web communities. 
A Web community is a cluster of densely linked pages representing a group of people with a special interest. 
Beyond explicit hyperlinks on the Web, links in other contexts are useful too, e.g., 
for discovering communities of named entities (e.g., people and organizations) in free text documents, and 
for analyzing social phenomena in emails.. 
Road map
Introduction
Social network analysis
Co-citation and bibliographic coupling 
PageRank
HITS
Summary
Social network analysis
Social network is the study of social entities (people in an organization, called actors), and their interactions and relationships. 
The interactions and relationships can be represented with a network or graph, 
each vertex (or node) represents an actor and 
each link represents a relationship. 
From the network, we can study the properties of its structure, and the role, position and prestige of each social actor. 
We can also find various kinds of sub-graphs, e.g., communities formed by groups of actors. 
Social network and the Web
Social network analysis is useful for the Web because the Web is essentially a virtual society, and thus a virtual social network, 
Each page: a social actor and 
each hyperlink: a relationship. 
Many results from social network can be adapted and extended for use in the Web context. 
We study two types of social network analysis, centrality and prestige, which are closely related to hyperlink analysis and search on the Web. 
Centrality
Important or prominent actors are those that are linked or involved with other actors extensively. 
A person with extensive contacts (links) or communications with many other people in the organization is considered more important than a person with relatively fewer contacts. 
The links can also be called ties. A central actor is one involved in many ties. 
Degree Centrality
Closeness Centrality
Betweenness Centrality 
If two non-adjacent actors j and k want to interact and actor i is on the path between j and k, then i may have some control over the interactions between j and k. 
Betweenness measures this control of i over other pairs of actors. Thus, 
if i is on the paths of many such interactions, then i is an important actor. 
Betweenness Centrality (cont …)
Undirected graph: Let pjk be the number of shortest paths between actor j and actor k. 
The betweenness of an actor i is defined as the number of shortest paths that pass i (pjk(i)) normalized by the total number of shortest paths. 
Betweenness Centrality (cont …)
Prestige 
Prestige is a more refined measure of prominence of an actor than centrality. 
Distinguish: ties sent (out-links) and ties received (in-links). 
A prestigious actor is one who is object of extensive ties as a recipient. 
To compute the prestige: we use only in-links. 
Difference between centrality and prestige: 
centrality focuses on out-links 
prestige focuses on in-links. 
We study three prestige measures. Rank prestige forms the basis of most Web page link analysis algorithms, including PageRank and HITS. 
Degree prestige 
Proximity prestige 
The degree index of prestige of an actor i only considers the actors that are adjacent to i. 
The proximity prestige generalizes it by considering both the actors directly and indirectly linked to actor i. 
We consider every actor j that can reach i. 
Let Ii be the set of actors that can reach actor i. 
The proximity is defined as closeness or distance of other actors to i. 
Let d(j, i) denote the distance from actor j to actor i. 
Proximity prestige (cont …)
Rank prestige 
In the previous two prestige measures, an important factor is considered, 
the prominence of individual actors who do the “voting” 
In the real world, a person i chosen by an important person is more prestigious than chosen by a less important person. 
For example, if a company CEO votes for a person is much more important than a worker votes for the person. 
If one’s circle of influence is full of prestigious actors, then one’s own prestige is also high. 
Thus one’s prestige is affected by the ranks or statuses of the involved actors. 
Rank prestige (cont …)
Based on this intuition, the rank prestige PR(i) is define as a linear combination of links that point to i: 
Road map
Introduction
Social network analysis
Co-citation and bibliographic coupling 
PageRank
HITS
Summary
Co-citation and Bibliographic Coupling 
Another area of research concerned with links is citation analysis of scholarly publications. 
A scholarly publication cites related prior work to acknowledge the origins of some ideas and to compare the new proposal with existing work. 
When a paper cites another paper, a relationship is established between the publications. 
Citation analysis uses these relationships (links) to perform various types of analysis. 
We discuss two types of citation analysis, co-citation and bibliographic coupling. The HITS algorithm is related to these two types of analysis. 
Co-citation
If papers i and j are both cited by paper k, then they may be related in some sense to one another. 
The more papers they are cited by, the stronger their relationship is.
Co-citation
Let L be the citation matrix. Each cell of the matrix is defined as follows: 
Lij = 1 if paper i cites paper j, and 0 otherwise. 
Co-citation (denoted by Cij) is a similarity measure defined as the number of papers that co-cite i and j, 



Cii is naturally the number of papers that cite i. 
A square matrix C can be formed with Cij, and it is called the co-citation matrix. 
Bibliographic coupling 
Bibliographic coupling operates on a similar principle. 
Bibliographic coupling links papers that cite the same articles
if papers i and j both cite paper k, they may be related.
The more papers they both cite, the stronger their similarity is.
Bibliographic coupling (cont …)
Road map
Introduction
Social network analysis
Co-citation and bibliographic coupling 
PageRank
HITS
Summary
PageRank
The year 1998 was an eventful year for Web link analysis models. Both the PageRank and HITS algorithms were reported in that year. 
The connections between PageRank and HITS are quite striking. 
Since that eventful year, PageRank has emerged as the dominant link analysis model, 
due to its query-independence, 
its ability to combat spamming, and 
Google’s huge business success. 
PageRank: the intuitive idea
PageRank relies on the democratic nature of the Web by using its vast link structure as an indicator of an individual page's value or quality. 
PageRank interprets a hyperlink from page x to page y as a vote, by page x, for page y. 
However, PageRank looks at more than the sheer number of votes; it also analyzes the page that casts the vote. 
Votes casted by “important” pages weigh more heavily and help to make other pages more "important." 
This is exactly the idea of rank prestige in social network. 
More specifically
A hyperlink from a page to another page is an implicit conveyance of authority to the target page. 
The more in-links that a page i receives, the more prestige the page i has. 
Pages that point to page i also have their own prestige scores. 
A page of a higher prestige pointing to i is more important than a page of a lower prestige pointing to i. 
In other words, a page is important if it is pointed to by other important pages. 
PageRank algorithm
According to rank prestige, the importance of page i (i’s PageRank score) is the sum of the PageRank scores of all pages that point to i. 
Since a page may point to many other pages, its prestige score should be shared. 
The Web as a directed graph G = (V, E). Let the total number of pages be n. The PageRank score of the page i (denoted by P(i)) is defined by:
Matrix notation
We have a system of n linear equations with n unknowns. We can use a matrix to represent them. 
Let P be a n-dimensional column vector of PageRank values, i.e., P = (P(1), P(2), …, P(n))T.
Let A be the adjacency matrix of our graph with



We can write the n equations with (PageRank)  
Solve the PageRank equation
This is the characteristic equation of the eigensystem, where the solution to P is an eigenvector with the corresponding eigenvalue of 1.  
It turns out that if some conditions are satisfied, 1 is the largest eigenvalue and the PageRank vector P is the principal eigenvector. 
A well known mathematical technique called power iteration can be used to find P. 
Problem: the above Equation does not quite suffice because the Web graph does not meet the conditions. 
Using Markov chain
To introduce these conditions and the enhanced equation, let us derive the same Equation (15) based on the Markov chain.
In the Markov chain, each Web page or node in the Web graph is regarded as a state. 
A hyperlink is a transition, which leads from one state to another state with a probability. 
This framework models Web surfing as a stochastic process. 
It models a Web surfer randomly surfing the Web as state transition.  
Random surfing
Recall we use Oi to denote the number of out-links of a node i. 
Each transition probability is 1/Oi if we assume the Web surfer will click the hyperlinks in the page i uniformly at random. 
The “back” button on the browser is not used and 
the surfer does not type in an URL. 
Transition probability matrix
Let A be the state transition probability matrix,,






Aij represents the transition probability that the surfer in state i (page i) will move to state j (page j). Aij is defined exactly as in Equation (14).
Let us start
Given an initial probability distribution vector that a surfer is at each state (or page) 
p0 = (p0(1), p0(2), …, p0(n))T (a column vector) and 
an nn transition probability matrix A, 
we have 





If the matrix A satisfies Equation (17), we say that A is the stochastic matrix of a Markov chain. 
Back to the Markov chain
In a Markov chain, a question of common interest is: 
Given p0 at the beginning, what is the probability that m steps/transitions later the Markov chain will be at each state j? 
We determine the probability that the system (or the random surfer) is in state j after 1 step (1 transition) by using the following reasoning: 
State transition
Stationary probability distribution
By a Theorem of the Markov chain, 
a finite Markov chain defined by the stochastic matrix A has a unique stationary probability distribution if A is irreducible and aperiodic. 
The stationary probability distribution means that after a series of transitions pk will converge to a steady-state probability vector  regardless of the choice of the initial probability vector p0, i.e., 
PageRank again
When we reach the steady-state, we have pk = pk+1 =, and thus 
			 =AT. 
 is the principal eigenvector of AT with eigenvalue of 1. 
In PageRank,  is used as the PageRank vector P. We again obtain Equation (15), which is re-produced here as Equation (22):
Is P =   justified?
Using the stationary probability distribution  as the PageRank vector is reasonable and quite intuitive because 
it reflects the long-run probabilities that a random surfer will visit the pages. 
A page has a high prestige if the probability of visiting it is high. 
Back to the Web graph
Now let us come back to the real Web context and see whether the above conditions are satisfied, i.e., 
whether A is a stochastic matrix and 
whether it is irreducible and aperiodic. 
None of them is satisfied. 
Hence, we need to extend the ideal-case Equation (22) to produce the “actual PageRank” model. 
A is a not stochastic matrix
A is the transition matrix of the Web graph




It does not satisfy equation (17)

because many Web pages have no out-links, which are reflected in transition matrix A by some rows of complete 0’s. 
Such pages are called the dangling pages (nodes). 
An example Web hyperlink graph
Fix the problem: two possible ways
Remove those pages with no out-links during the PageRank computation as these pages do not affect the ranking of any other page directly. 
Add a complete set of outgoing links from each such page i to all the pages on the Web. 
A is a not irreducible
Irreducible means that the Web graph G is strongly connected. 
Definition: A directed graph G = (V, E) is strongly connected if and only if, for each pair of nodes u, v ∈ V, there is a path from u to v.
A general Web graph represented by A is not irreducible because 
for some pair of nodes u and v, there is no path from u to v. 
In our example, there is no directed path from nodes 3 to 4.  
A is a not aperiodic
A state i in a Markov chain being periodic means that there exists a directed cycle that the chain has to traverse. 
Definition: A state i is periodic with period k > 1 if k is the smallest number such that all paths leading from state i back to state i have a length that is a multiple of k. 
If a state is not periodic (i.e., k = 1), it is aperiodic. 
A Markov chain is aperiodic if all states are aperiodic.
An example: periodic 
Fig. 5 shows a periodic Markov chain with k = 3. Eg, if we begin from state 1, to come back to state 1 the only path is 1-2-3-1 for some number of times, say h. Thus any return to state 1 will take 3h transitions. 
Deal with irreducible and aperiodic
It is easy to deal with the above two problems with a single strategy. 

Add a link from each page to every page and give each link a small transition probability controlled by a parameter d. 

Obviously, the augmented transition matrix becomes irreducible and aperiodic 
Improved PageRank
After this augmentation, at a page, the random surfer has two options
With probability d, he randomly chooses an out-link to follow.
With probability 1-d, he jumps to a random page
Equation (25) gives the improved model, 


	where E is eeT (e is a column vector of all 1’s) and thus E is a nn square matrix of all 1’s. 

Follow our example
The final PageRank algorithm
(1-d)E/n + dAT is a stochastic matrix (transposed). It is also irreducible and aperiodic
If we scale Equation (25) so that eTP = n, 


PageRank for each page i is 
The final PageRank (cont …) 
(28) is equivalent to the formula given in the PageRank paper


The parameter d is called the damping factor which can be set to between 0 and 1. d = 0.85 was used in the PageRank paper. 
Compute PageRank
Use the power iteration method
Advantages of PageRank
Fighting spam. A page is important if the pages pointing to it are important. 
Since it is not easy for Web page owner to add in-links into his/her page from other important pages, it is thus not easy to influence PageRank. 
PageRank is a global measure and is query independent. 
PageRank values of all the pages are computed and saved off-line rather than at the query time. 
Criticism: Query-independence. It could not distinguish between pages that are authoritative in general and pages that are authoritative on the query topic. 
Road map
Introduction
Social network analysis
Co-citation and bibliographic coupling 
PageRank
HITS
Summary
HITS
HITS stands for Hypertext Induced Topic Search. 
Unlike PageRank which is a static ranking algorithm, HITS is search query dependent. 
When the user issues a search query, 
HITS first expands the list of relevant pages returned by a search engine and 
then produces two rankings of the expanded set of pages, authority ranking and hub ranking. 
Authorities and Hubs
Authority: Roughly, a authority is a page with many in-links. 
The idea is that the page may have good or authoritative content on some topic and 
thus many people trust it and link to it. 
Hub: A hub is a page with many out-links. 
The page serves as an organizer of the information on a particular topic and 
points to many good authority pages on the topic. 
Examples
The key idea of HITS
A good hub points to many good authorities, and 
A good authority is pointed to by many good hubs. 
Authorities and hubs have a mutual reinforcement relationship. Fig. 8 shows some densely linked authorities and hubs (a bipartite sub-graph). 
The HITS algorithm: Grab pages
Given a broad search query, q, HITS collects a set of pages as follows:
It sends the query q to a search engine. 
It then collects t (t = 200 is used in the HITS paper) highest ranked pages. This set is called the root set W. 
It then grows W by including any page pointed to by a page in W and any page that points to a page in W. This gives a larger set S, base set. 
The link graph G
HITS works on the pages in S, and assigns every page in S an authority score and a hub score. 
Let the number of pages in S be n. 
We again use G = (V, E) to denote the hyperlink graph of S. 
We use L to denote the adjacency matrix of the graph. 
The HITS algorithm
Let the authority score of the page i be a(i), and the hub score of page i be h(i). 
The mutual reinforcing relationship of the two scores is represented as follows:
HITS in matrix form
We use a to denote the column vector with all the authority scores, 
		a = (a(1), a(2), …, a(n))T, and 
use h to denote the column vector with all the authority scores, 
		h = (h(1), h(2), …, h(n))T,
Then, 
		a = LTh 
		h = La 
Computation of HITS
The computation of authority scores and hub scores is the same as the computation of the PageRank scores, using power iteration. 
If we use ak and hk to denote authority and hub vectors at the kth iteration, the iterations for generating the final solutions are 
The algorithm
Relationships with co-citation and bibliographic coupling 
Recall that co-citation of pages i and j, denoted by Cij, is 

the authority matrix (LTL) of HITS is the co-citation matrix C 
bibliographic coupling of two pages i and j, denoted by Bij is 

the hub matrix (LLT) of HITS is the bibliographic coupling matrix B 
Strengths and weaknesses of HITS 
Strength: its ability to rank pages according to the query topic, which may be able to provide more relevant authority and hub pages. 
Weaknesses:
It is easily spammed. It is in fact quite easy to influence HITS since adding out-links in one’s own page is so easy. 
Topic drift. Many pages in the expanded set may not be on topic. 
Inefficiency at query time: The query time evaluation is slow. Collecting the root set, expanding it and performing eigenvector computation are all expensive operations 
Road map
Introduction
Social network analysis
Co-citation and bibliographic coupling 
PageRank
HITS
Summary
Summary
In this chapter, we introduced 
Social network analysis, centrality and prestige
Co-citation and bibliographic coupling 
PageRank, which powers Google
HITS
Yahoo! and MSN have their own link-based algorithms as well, but not published. 
Important to note: Hyperlink based ranking is not the only algorithm used in search engines. In fact, it is combined with many content based factors to produce the final ranking presented to the user. 
Summary
Links can also be used to find communities, which are groups of content-creators or people sharing some common interests. 
Web communities
Email communities
Named entity communities
Focused crawling: combining contents and links to crawl Web pages of a specific topic. 
Follow links and
Use learning/classification to determine whether a page is on topic. 
Big Data Overview
Dr. Anil Maheshwari

Small Data vs Big Data
Big Data defined
Collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools. 
Challenges include capture, curation, storage, search, sharing, analysis, and visualization
Additional information is derivable from analysis of a single large set of related data, as compared to separate smaller sets
to spot business trends, determine quality of research, prevent diseases, link legal citations, combat crime, and determine real-time roadway traffic conditions
The ‘Big Data’ Picture - Business
“Big data will disrupt your business. Your actions will determine whether these disruptions are positive or negative.” (Source: Big Data Disruptions Tamed With Enterprise Architecture” Gartner, 2012)
Business Issues
How to use generated data as a strategic asset in real-time, to identify opportunities, thwart threats and achieve operational efficiencies
How to organize the business to not get buried in high volume, velocity and variety of data
How to design a ‘Digital Business Strategy’ around digital assets and capabilities


The ‘Big Data’ Picture - Technology
"Big data" forces organizations to address the variety of information assets and how fast these new asset types are changing information management demands. (Source: 'Big Data' and Content Will Challenge IT Across the Board” Gartner, 2012)
IT Issues
How can IT professionals integrating "big data" structured assets with content must increase their business requirement identification skills.
How can IT support teams support end-user-deployed big data solutions
How to re-design enterprise data warehouses to address big data issues
How to help design a ‘Digital Business Strategy’ around digital assets and capabilities


Films on Big Data
Digital nation – PBS (90 min)
http://video.pbs.org/video/1402987791/ig 
BBC Documentary on Big Data (60 min)
https://www.youtube.com/watch?v=tI-xxEur07Q

Big Data: the exponential growth of business data
This growth is made possible in large part by the advancement of technology. This graph shows growth of disk drive average capacities: 
from 1MB in 1980 to 1TB in 2010.
  

Big Data characteristics
Variety
Many types, sources, and quality
Velocity
Mobile + Social Media = Velocity
Volume
Autonomous data streams of video, audio, text, data, etc.
Sources of Data:
People: Google searches, Facebook posts, Tweets, Youtube videos, other Social Media, blogs, emails
Machines: RFID, telematics, connected devices, Mobile location, surveillance, etc
Metadata: web crawlers, bots







Innovation with Big Data
Use of Big Data
Discover new insights
What experts think is ordinary can be very interesting for ordinary folks, and vice versa
Create new models of reality
Refuting the taken for granted is interesting
Reality seems to be X, but in actuality reality is X’






Key findings highlighted in IBM Analytics study
Across all industries, the business case for big data is strongly focused on addressing customer-centric objectives
A scalable and extensible information management foundation is a prerequisite for big data advancement
Organizations are beginning their pilots and implementations by using existing and newly accessible internal sources of data
Advanced analytic capabilities are required, yet lacking, for organizations to get most value from big data
As awareness and involvement in big data grows, four key stages of big data adoption emerge along a continuum.

Insights into Big Data
The faster you analyze the data, the more its predictive value
Maintain one copy of your data, not multipl
Use more diverse data, not just more data
Data has value beyond what you initially anticipate
Plan for exponential growth
Solve a real pain-point
Put humans and data together to get most insight
Big data is transforming business, just like IT did




Recommendations for Big Data
Educate business and IT leaders about the business benefits of sophisticated analytics
Analyze, identify and remove cultural roadblocks to data sharing
Identify talent pool and skill gaps for interdisciplinary roles such as data scientists and chief data officers.
Create an enterprise architecture and acquire the toolsets to manage and process data thru the life cycle.
Start experiments with Map-Reduce clusters, and business personalization themes 
Organize and consolidate data with minimum information loss
Process in near real-time or the advantage is lost 
Dynamically keep looking for new patterns


Big Data imperatives
Organize and consolidate data with minimum information loss
Process in near real-time or the advantage is lost 
Dynamically keep looking for new patterns

Big Data Growth drivers
Many new sources of data
People: Google searches, Facebook posts, Tweets, other Social Media, blogs, emails
Machines: RFID, telematics, connected devices, Mobile location, surveillance, etc
Metadata: web crawlers, bots, 
High density: high def videos, 


Big Data economics
Data growing faster than Moore’s law
Doubling every 12-18 months
More data generated in 1 second today than was on the whole web in 1990
McKinsey 2011 report on Big Data oppty
 Large business opportunity
160,000 new jobs
Davenport books/articles 2006-12
Competing on analytics
‘Data scientist’ – sexiest job of the future 

Big Data technology
Non-relational data structures
Hadoop and open source stack
Google BigFile, Semantic web, etc.
Massively parallel computing
Map-Reduce algorithms
Unstructured Information Management Architecture (UIMA)
The ‘sauce’ behind IBM’ Watson system
Natural language processing





Lots of Data Scientist Jobs:  WordCloud
Big Data perspectives
Economist magazine http://www.economist.com/blogs/dailychart/2011/11/big-data-0
Churchill club seminar http://www.youtube.com/watch?v=KD_g6byn83s
BBC Horizon on Big Data
https://www.youtube.com/watch?v=tI-xxEur07Q
Write down 2-4 main points
Hadoop & MapReduce
Dr. Anil Maheshwari
Hadoop & MapReduce defined
Hadoop is non-relational system of distributed and cost-effective data storage on commodity hardware
MapReduce is a framework for parallel processing with minimal movement of data and near-realtime results

Cluster Computing
Scale-out architecture using commodity hardware
Each node serve as data and processing server
Three Challenges
Node failure (MTBF = 3 years)
Network (1Gb/sec) bottleneck
Distributed Programming


MapReduce is the solution
Store data redundantly on multiple nodes
Move computation close to data
Provide simple programming model to hide the underlying complexity

Hadoop architecture: Chunking the data
Divide files into chunks (64MB each)
Store each chunk on multiple nodes




One chunk server is master, others are slaves
MapReduce architecture
Two kinds of jobs:
Mappers – map the job into many tasks
Reducers – combine the inputs from the mappers into a single output
Jobs/tasks tracked in master-slave mode

Master-slave in MapReduce and Hadoop

Hadoop Distributed Architecture
MapReduce operation
MapReduce processing is similar to UNIX sequence (also called pipe) structure 
e.g. the UNIX command: 
grep | sort | count  myfile.txt
will produce a wordcount in the text document called myfile.txt.

Map-Reduce Processing – example2
MapReduce Architecture
WordCount: Map and Reduce
WordCount Exercise  using MapReduce 
– step 1
MapReduce step 2 – Map complete
MapReduce step 3 - Sort
MapReduce – step 4 
-Reduce provides final output
Java code for Wordcount application
map(String key, String value): 
for each word w in value: 
EmitIntermediate(w, "1");

reduce(String key, Iterator values): 
int result = 0; 
for each v in values: 
result += ParseInt(v); 
Emit(AsString(result));

Next steps
Watch Cloudera CEO: https://www.youtube.com/watch?v=S9xnYBVqLws
Watch Horton Founder: https://www.youtube.com/watch?v=ht3dNvdNDzI
Watch Coursera – Stanford videos
Read Map-Reduce tutorial https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html


Big Data Sources, Applications, Architectures
Dr. Anil Maheshwari
Big Data Sources
People to People Communications
Social Media
People to Machine Communications
Web access 
Machine to Machine (M2M) Communications 
RFID tags
Sensors

People to People Commns
Old and new media; 1-1 and social media
Listening Platforms to filter and analyze
Video, Audio, more variety

People to Machine Commns
Web logs
Digital assistants
Mobile phone movement records
Mobile personal fitness devices
Machine to Machine Commns
Sensors and Trackers
RFID devices
Internet of Things – a trillion devices
Listening Platforms to filter and analyze

Big Data Applications

Monitoring and Tracking Applications
Public Health Monitoring
Consumer Sentiment Monitoring
Asset tracking
Supply chain monitoring
Electricity Consumption tracking
 Equipment Health tracking for Preventive Machine Maintenance 
Preventive Machine Maintenance 
Analysis and Insight Applications
Predictive Policing
Winning Political Elections 
Personal Health
New Product Development
Flexible auto insurance 
Location-based retail promotion 
Recommendation service





Big Data Ecosystem/Architecture
Layers in Big Data Architecture
Data Sources – P2P, P2M, M2M comm, Biz ops
Data Ingest – Apache Kafka
Streaming Analysis – Apache Spark
Batch Analysis  - Apache MapReduce
Data Organizing Layer – NoSQL e.g. Apache HBase
Distributed File System Layer - HDFS
Infrastructure layer – Cloud computing
Data Consumption Layer – Data Mining etc
Google Query
IBM Watson Architecture
Netflix Architecture
Vmware architecture
Weather company architecture
Ticketmaster Architecture
LinkedIn Architecture
PayPal Architecture

Thanks. 
Watch Cloudera CEO: https://www.youtube.com/watch?v=S9xnYBVqLws
Watch Horton Founder: https://www.youtube.com/watch?v=ht3dNvdNDzI
Stream Computing
Anil Maheshwari

Streaming applications
Mining query streams – which queries are more frequent today
Mining click streams – which urls are clicked more frequently this hr
Mining IP-switched netowrks – identifying denial-of-service attacks





Streaming concepts
High and unpredictable rate of ingest of data – programmer control vs random people
Data is too large to store in memory
Do simple processing on the stream with minimal computations
Count average and count and max in a stream
Count the number of 1s in a stream of 0s and 1s
Count number of distinct items in a stream
Analyze a sliding window of the stream (time or data elements)
Old elements slide out of the window
Sample the stream to get approximate results




algorithms
Compute properties without storing the entire stream
Limit the number of arithmetic steps per new data element
N is so large that we want to store only O(log N). 
Filter the stream to reduce the number of things to do
Efficiency is of the essence
Ad-hoc and Standing queries




Bloom Filter
Can show if a url has been seen before
Uses a long String of 1s and a bunch of hash functions
Hash every new input using all the hash functions to set the bits in the string of 1s
If the new input hashes to all 1s in the string, then that input has been seen before
Creates some false positives, but no false negatives. 




Apache Spark for Stream computing
Apache Spark is an integrated, fast, in-memory, general-purpose engine for large-scale data processing. Spark is ideal for iterative and interactive processing tasks on large data sets and streams. 
Spark achieves 10-100x performance over Hadoop by operating with an in-memory construct called ‘Resilient Distributed Datasets’ (RDDs), which help avoid the latencies involved in disk reads and writes. 
Spark allows programmers to develop complex, multi-step data pipelines using directed acyclic graph (DAG) pattern. 
Spark is written mostly in Scala, a high-level language. Spark’s built-in libraries (for Machine Learning, Graph Processing, Stream processing, SQL) deliver seamless fast data processing along with high programmer productivity. 
Spark has become a more efficient and productive alternative for Hadoop ecosystem, and is increasing being used in industry. It is compatible with Hadoop file systems and tools. 

Apache Spark Architecture
Spark vs Hadoop
Spark RDDs
RDD, Resilient Distributed Datasets, are immutable and partitioned collection of records. They keep data in memory and can improve performance by an order of magnitude. 
RDDs can only be created by (a) reading data from a stable storage such as HDFS or (b) transformations on existing RDDs.
Spark API permit two major types of operation on RDDs: transformations and actions. Transformations include map(), filter(), sample(), and union()






Spark code for Pagerank
//load the edges as a graph object
val graph = GraphLoader.edgeListFile(sc, "outlink.txt")
// Run pagerank
val ranks = graph.pagerank(0.00000001).vertices


// join the rank with the webpages
val pages = sc.textFile("pages.txt").map{line => val fields = line.split(",") (fields(0).toLong, fields(1)) }
val ranksByPagename = pages.join(ranks).map { case (id, (pagename, rank)) => (pagename, rank)}
//print the output
println(rankByPagename.collect().mkString("\n"))

Web Log Analyzer application

A web log analyzer is an automated software tool that helps to analyze web application server logs and make decisions on a number of issues.
The application receives streaming logs from a server location, and analyzes multiple things using many algorithms to generate the desired results, including alerting the system administrator as needed. 
This system provides the following major functions:
Calculate content size
Count Response code
Analyze requesting IP-address
Manage End points






Web Log Analyzer architecture
Technology Stack
Apache Spark v2
Hadoop 2.6.0 cdh5
Apache Flume
Scala, Java
MongoDB
RestFul Webservices
Front UI tools 
Linux Shell Scripts

Application code
(https://github.com/databricks/reference-apps/blob/master/logs_analyzer/chapter1/scala/src/main/scala/com/databricks/apps/logs/chapter1/LogAnalyzer.scala)
---
NoSQL DB

Dr. Anil Maheshwari
NoSQL Databases in Big Data Architecture
NoSQL Databases
NoSQL databases are next-generation databases that are non-relational in their design. 
 NoSQL is useful when an enterprise needs to access, analyze and utilize massive amounts of either structured or unstructured data
The constraints of a relational database are relaxed in many ways. Thus there are many types of NoSQL databases
NoSQL database files are written once and almost never updated in place. 

NoSQL vs RDBMS
CAP Theorem
CAP theorem states that in any distributed data store, one can choose only two out of the three - Consistency, Availability and Partition Tolerance
Need to make a choice between Consistency and Availability: i.e. a Trade-off between accuracy and low latency. 
Data will be eventually consistent across all partitions, but not at any moment. 
NoSQL Architecture
Types of NoSQL Databases
Key-Pair – e.g Amazon SimpleDB, 
Columnar Family – Cassandra, HBase
Document –  e.g. MongoDB
Graph – e.g. Neo4J
Popular NoSQL architectures
Master-Slave  … e.g. HBase
Ring Architecture … e.g. Cassandra

NoSQL access languages - Hive
Hive is a declarative SQL-like language for queries. It is best suited for structured data such as that stored in Hbase, a key-value store.
 Hive data Columns are mapped to tables in HDFS. This mapping is stored in Metadata. 

NoSQL access languages - Pig
Next steps
Watch Cloudera CEO: https://www.youtube.com/watch?v=S9xnYBVqLws

Stream Computing

Anil Maheshwari
Stream Processing in Big Data Architecture

Stream Computing defined
Processing large quantities of data from a vast set of extremely fast incoming data streams. 
Mining data in motion
Compute simple approximate metrics in real time
 relax many computational accuracy requirements




Streaming concepts
High and unpredictable rate of ingest of data – programmer control vs random people
Data is too large to store in memory
Do simple processing on the stream with minimal computations
Count average and count and max in a stream
Count the number of 1s in a stream of 0s and 1s
Count number of distinct items in a stream
Analyze a sliding window of the stream (time or data elements)
Old elements slide out of the window
Sample the stream to get approximate results




Streaming applications
Mining query streams 
 which queries are more frequent today
Mining click streams 
 which urls are clicked more frequently this hr
Mining IP-switched networks 
identifying denial-of-service attacks





Algorithms
Compute properties without storing the entire stream
Limit the number of arithmetic steps per new data element
N is so large that we want to store only O(log N). 
Filter the stream to reduce the number of things to do
Efficiency is of the essence
Ad-hoc and Standing queries




Bloom Filter
Can show if a url has been seen before
Uses a long String of 1s and a few of hash functions
Hash every new input using all the hash functions to set the bits in the string of 1s
If the new input hashes to all 1s in the string, then that input has been seen before
Creates some false positives, but no false negatives. 




Apache Spark for Stream computing
Apache Spark is an integrated, fast, in-memory, general-purpose engine for large-scale data processing. 
Spark is ideal for iterative and interactive processing tasks on large data sets and streams. 
Spark achieves 10-100x performance over Hadoop by operating with an in-memory construct called ‘Resilient Distributed Datasets’ (RDDs), which help avoid the latencies involved in disk reads and writes. 
Spark allows programmers to develop complex, multi-step data pipelines using directed acyclic graph (DAG) pattern. 
Spark is written mostly in Scala, a high-level language. Spark’s built-in libraries (for Machine Learning, Graph Processing, Stream processing, SQL) deliver seamless fast data processing along with high programmer productivity. 
Spark has become a more efficient and productive alternative for Hadoop. It is compatible with Hadoop file systems and tools. 

Apache Spark Architecture
Spark vs Hadoop
Spark RDDs
RDD, Resilient Distributed Datasets, are immutable and partitioned collection of records. They keep data in memory and can improve performance by an order of magnitude. 
RDDs can only be created by (a) reading data from a stable storage such as HDFS or (b) transformations on existing RDDs.
Spark API permit two major types of operation on RDDs: transformations and actions. Transformations include map(), filter(), sample(), and union()







Spark code for Pagerank
//load the edges as a graph object
val graph = GraphLoader.edgeListFile(sc, "outlink.txt")
// Run pagerank
val ranks = graph.pagerank(0.00000001).vertices


// join the rank with the webpages
val pages = sc.textFile("pages.txt").map{line => val fields = line.split(",") (fields(0).toLong, fields(1)) }
val ranksByPagename = pages.join(ranks).map { case (id, (pagename, rank)) => (pagename, rank)}
//print the output
println(rankByPagename.collect().mkString("\n"))

Web Log Analyzer application

A web log analyzer is an automated software tool that helps to analyze web application server logs and make decisions on a number of issues.
The application receives streaming logs from a server location, and analyzes multiple things using many algorithms to generate the desired results, including alerting the system administrator as needed. 
This system provides the following major functions:
Calculate content size
Count Response code
Analyze requesting IP-address
Manage End points






Web Log Analyzer architecture
Technology Stack
Apache Spark v2
Hadoop 2.6.0 cdh5
Apache Flume
Scala, Java
MongoDB
RestFul Webservices
Front UI tools 
Linux Shell Scripts

Application code
(https://github.com/databricks/reference-apps/blob/master/logs_analyzer/chapter1/scala/src/main/scala/com/databricks/apps/logs/chapter1/LogAnalyzer.scala)
---
Cloud Computing
Cloud computing is a cost-effective and flexible mode of delivering IT infrastructure as a service to clients, over internet, on a metered basis. 
The cloud computing model offers clients enormous flexibility to use as much IT capacity – compute, storage, network –  as needed without having to invest in a dedicated IT capacity on one’s own. 
IT capacity can be purchased as a custom package depending upon range of services and capacity requirements. 
The computing cloud is the ultimate cosmic computer aligned with all laws of nature. 

Cloud Computing Access Model 
Cloud Computing as virtualized infrastructure
Benefits of Cloud Computing
Flexible Capacity: The capacity can scale up rapidly. One can expand and reduce resources according to one’s specific service requirements, as and when needed.  
Attractive payment model: Cloud computing works on a pay-per-use model. i.e. one pays only for what one uses, and for how long one uses it.  IT costs become an expense rather than a capital expense for the client. 
Resiliency and Security: The failure of any individual server and storage resources does not impact the user.  The Servers and storage for all clients are all backed up, and isolate to maximize security of data. 
Models of Cloud Computing – by ownership and range of services
Public Cloud
Private Cloud
Hybrid Cloud

Infrastructure as a service
Platform as a service
Application as a service
Data Ingest-kafka
Anil Maheshwari

Data Ingest System
A Data ingesting system is a reliable and efficient point of reception for all data coming into a system. 
Fast and flexible buffer for receiving and storing all incoming streams of data.  The data in the buffer is stored in a sequential manner, 
Data is made available to all consuming applications in a fast and orderly manner. 

Messaging systems
Point-to-point systems
Well understood, but inefficient for high volumes
Publish-Subscribe systems
Brokers navigate relationships between data producers and consumers
Apache Kafka is one such system
Flume is another, closely tied to Hadoop

Apache Kafka architecture
Kafka components
Producers
Consumers
Brokers
Topics
Zookeeper – publishes metadata

Data is stored and consumed sequentially
Key Attributes of Kafka
Disk based: Kafka works on a cluster of disks. It does not keep everything in memory, and keeps writing to the disk to make the storage permanent.
Fault tolerant: Data in Kafka is replicated across multiple brokers. When any leader broker fails, a follower broker takes over as leader and everything continues to work normally.
Scalable: Kafka can scale up easily by adding more partitions or more brokers. More brokers help to spread the load and this provides greater throughput.
Low latency: Kafka does very little processing on the data. Thus it has very low latency rate Messages produced by the consumer are published and available to the consumer within a few milliseconds. 
Finite Retention:  Kafka by default keeps the message in the cluster for a week. After that the storage is refreshed. Thus the data consumers have up to a week to catch up on data.
